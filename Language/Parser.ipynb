{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parser.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsQDfuv5rY0pYHsaUf8ujY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitbhavnani/Aritificial-Intelligence/blob/master/Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmP1ZcgDgwbP"
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o6Sil-Sg6Y-"
      },
      "source": [
        "TERMINALS = \"\"\"\n",
        "Adj -> \"country\" | \"dreadful\" | \"enigmatical\" | \"little\" | \"moist\" | \"red\"\n",
        "Adv -> \"down\" | \"here\" | \"never\"\n",
        "Conj -> \"and\"\n",
        "Det -> \"a\" | \"an\" | \"his\" | \"my\" | \"the\"\n",
        "N -> \"armchair\" | \"companion\" | \"day\" | \"door\" | \"hand\" | \"he\" | \"himself\"\n",
        "N -> \"holmes\" | \"home\" | \"i\" | \"mess\" | \"paint\" | \"palm\" | \"pipe\" | \"she\"\n",
        "N -> \"smile\" | \"thursday\" | \"walk\" | \"we\" | \"word\"\n",
        "P -> \"at\" | \"before\" | \"in\" | \"of\" | \"on\" | \"to\" | \"until\"\n",
        "V -> \"arrived\" | \"came\" | \"chuckled\" | \"had\" | \"lit\" | \"said\" | \"sat\"\n",
        "V -> \"smiled\" | \"tell\" | \"were\"\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6txpn6Wg6iM"
      },
      "source": [
        "NONTERMINALS = \"\"\"\n",
        "S -> NP VP | S Conj S | NP VP Conj VP\n",
        "AP -> Adj | Adj AP\n",
        "NP -> N | Det NP | AP NP | NP PP\n",
        "PP -> P NP | P S\n",
        "VP -> V | V NP | V NP PP | V PP | VP Adv | Adv VP\n",
        "\"\"\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjZyldhwg6rt"
      },
      "source": [
        "grammar = nltk.CFG.fromstring(NONTERMINALS + TERMINALS)\n",
        "parser = nltk.ChartParser(grammar)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-jU7Hc9hHmC"
      },
      "source": [
        "def main():\n",
        "\n",
        "    # If filename specified, read sentence from file\n",
        "    if len(sys.argv) == 2:\n",
        "        with open(sys.argv[1]) as f:\n",
        "            s = f.read()\n",
        "\n",
        "    # Otherwise, get sentence as input\n",
        "    else:\n",
        "        s = input(\"Sentence: \")\n",
        "\n",
        "    # Convert input into list of words\n",
        "    s = preprocess(s)\n",
        "\n",
        "    # Attempt to parse sentence\n",
        "    try:\n",
        "        trees = list(parser.parse(s))\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "        return\n",
        "    if not trees:\n",
        "        print(\"Could not parse sentence.\")\n",
        "        return\n",
        "\n",
        "    # Print each tree with noun phrase chunks\n",
        "    for tree in trees:\n",
        "        tree.pretty_print()\n",
        "\n",
        "        print(\"Noun Phrase Chunks\")\n",
        "        for np in np_chunk(tree):\n",
        "            print(\" \".join(np.flatten()))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H_Bl8-YhII7"
      },
      "source": [
        "def preprocess(sentence):\n",
        "    \"\"\"\n",
        "    Convert `sentence` to a list of its words.\n",
        "    Pre-process sentence by converting all characters to lowercase\n",
        "    and removing any word that does not contain at least one alphabetic\n",
        "    character.\n",
        "    \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return [word for word in words if re.match('[a-z]', word)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuBv5UgahHRU"
      },
      "source": [
        "def np_chunk(tree):\n",
        "    \"\"\"\n",
        "    Return a list of all noun phrase chunks in the sentence tree.\n",
        "    A noun phrase chunk is defined as any subtree of the sentence\n",
        "    whose label is \"NP\" that does not itself contain any other\n",
        "    noun phrases as subtrees.\n",
        "    \"\"\"\n",
        "    return [subtree for subtree in tree.subtrees(is_np_chunk)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ZsrbnwheFd"
      },
      "source": [
        "def is_np_chunk(tree):\n",
        "    \"\"\"\n",
        "    Returns true if given tree is a NP chunk.\n",
        "    A noun phrase chunk is defined as any subtree of the sentence\n",
        "    whose label is \"NP\" that does not itself contain any other\n",
        "    noun phrases as subtrees.\n",
        "    \"\"\"\n",
        "    if tree.label() == 'NP' and \\\n",
        "            not list(tree.subtrees(lambda t: t.label() == 'NP' and t != tree)):\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}
